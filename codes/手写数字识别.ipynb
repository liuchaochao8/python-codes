{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa1a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d38c997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x160bfedd2f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "torch.__version__\n",
    "#重要参数\n",
    "n_epochs = 3 #迭代轮数\n",
    "batch_size_train = 64 #训练集规模\n",
    "batch_size_test = 1000 #测试集规模\n",
    "learning_rate = 0.01 #学习率\n",
    "\n",
    "#一般参数\n",
    "momentum = 0.5 #动量系数\n",
    "log_interval = 10 #控制可视化输出间隔\n",
    "\n",
    "random_seed = 1 \n",
    "#是否使用非确定性算法，True时会自动寻找适合当前的高效算法。\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.manual_seed(random_seed) #设置随机种子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11e18a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True, #MNIST下载数据\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "    batch_size=batch_size_test, shuffle=True)\n",
    "examples = enumerate(train_loader) #获取一个batch（样本数量，通道，28,28维）\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    "torch.Size([64, 1, 28, 28])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2aac458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAELCAYAAAARNxsIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAclElEQVR4nO3deZRU1bXH8d8WFIOARBGccIriUhNFRYkogziiD2KQOD0F8SlO0eUUMTHKgyAKJkGND5fPhTjxokZFgzMqJEHFEEQIBJwIiCAiGlCgBU2f90cVN/fcWNVV1aeqbjXfz1q91t6cW/ec7jr0rjv0ueacEwAAIWxR7QEAAJoOigoAIBiKCgAgGIoKACAYigoAIBiKCgAgmCZdVMxsDzNzZta8Cn0vNrNjK90vwmDuoFSb+9xpdFExszPM7A0zW2dmK7PxJWZmIQZYLma2NvZVb2Z1sfw/i9zXfWY2skzjnJCdoHuXY//VxNwpz9wxs8vM7O9m9rmZ/cXMjgq5/zRg7qR37jSqqJjZ1ZJul3SrpB0ldZB0kaQjJW2V4zXNGtNnKM65Vpu+JH0gqW/s3yZu2q4anzZifR8l6TvV6r+cmDvlYWZdJd0iaYCkbSWNlzQpLT+7EJg75RFs7jjnSvrKdrpO0qkNbHefpLskPZvd/lhJ+0maJmm1pPmS+sW2nybp/Fh+rqTpsdwpM4HelfQPSf8jybJtzST9UtIqSYskXZrdvnkDY1ws6dhs3EvSh5KGSloh6cHkGGLj2FvSEElfSdooaa2kybF9XiNprqQ1kh6RtHURP9/mkmZLOnBTX6W+V2n7Yu6Ub+5IOl3Sn2P5Ntn+dqr2+87c2TzmTmOOVI6Q1ELSUwVse5akmyS1lvSGpMmSXpTUXtJlkiaa2b5F9P0fkg6TdJCk0ySdkP33C7JtB0vqokzFLcWOkraTtLsyb15Ozrn/lTRR0hiX+bTRN9Z8mqQTJe2pTHE4d1ODma1u4NDySkl/dM7NLek7SDfmjso2d56T1MzMumY/YZ4n6S1lflE1BcwdpXvuNKaotJO0yjn39aZ/MLPXsoOuM7MesW2fcs696pyrl9RZUitJtzjnNjrnXpH0tKQzi+j7FufcaufcB5KmZvcpZX6YtznnljrnPpN0c4nfW72kYc65Dc65uhL3IUl3OOeWZ8cyOTZOOefaOuemf9OLzKyjpAsl3diIvtOMudOwkuaOpC8kPS5puqQNkoZJGuKyHz2bAOZOw6o6dxpTVD6V1C5+7s8518051zbbFt/30li8s6Sl2Td6kyWSdimi73jlXK/MZIn2ndhvKT5xzn1Z4mvjco2zIbdJGuGcWxNgDGnE3GlYqXPnfGU+YR6gzPWFsyU9bWY7BxhTGjB3GlbVudOYovK6MtXsBwVsG690yyV1NLN437tJWpaN10lqGWvbsYgxfSSpY2K/pUhWZm9MZpYcU+hPgcdIutXMVpjZpgnyupmdFbifamHu5N6+sQ5S5vz6O865eufc88p8b90C91MtzJ3c2zdWkLlTclFxzq2WNFzSODMbYGatzGwLM+uszAWeXN5Q5od1rZltaWa9JPWV9HC2/S1J/c2sZfY22v8qYliPSrrczHY1s29Luq6I1+YzR9IBZtbZzLaW9N+J9o8l7RWoL0nqpMwb3Fn/OnTtK2lSwD6qhrnjCT13Zko62cz2sozjlJlP8wL2UTXMHU8q506jbil2zo2RdJWkayWtVOabvFuZOxhey/GajZL6SeqjzN0S4yQNdM4tzG4yVpk7Gj6WdL8yF6MKdY+kF5R5M96U9ERx39E3c869I2mEpJeUufsjeU5yvKT9s+d1nyxkn9n70rvn6G+lc27Fpq/sP69q5HnWVGHuRILOHUkPKPOLcpqkzyXdIenC2M+o5jF3IqmcO5tuiQMAoNGa9DItAIDKoqgAAIKhqAAAgqGoAACCoagAAIIpaiVMM+NWsRRyzqV9uW/mTTqtcs7tUO1B5MPcSa2cc4cjFWDzVepyIkDOuUNRAQAEQ1EBAARDUQEABENRAQAEQ1EBAARDUQEABENRAQAEQ1EBAARDUQEABENRAQAEQ1EBAARDUQEABFPUKsXIGDduXBQ/8MADXtuMGTMqPRw00pZbbunlhx9+eBQ///zzXlurVq28/I9//GMUDxs2zGubOXOml69bt65R4wRqAUcqAIBgKCoAgGA4/VWCLl26RPGCBQu8Nk5/1Z6zzz7by8ePH59zW+f8Z0b16NEjil955RWvbc2aNV7et2/fKJ4+fXrR40TT0qFDhyjec889vbbvfe97Xj5w4MAoTp5yv+eee8owutJxpAIACIaiAgAIhqICAAiGayoliF9TefDBB6s4EpSiZ8+eXn7rrbeWpZ9tt93Wy++4444oPuSQQ8rSJyqreXP/V2j37t2juH///l5br169vHz77beP4p122qngPg888EAv/9Of/hTFb7/9tteWvAZYCRypAACCoagAAILh9FcBkqdL4v7whz9UcCQIYeTIkV6+3XbbBdnv119/7eXz5s3z8oULFwbpB+nx4x//2MvHjh1b8Gvjp6amTp2ad9uXX345iuO3Ikv+nzUkb0VOzsFK4EgFABAMRQUAEAxFBQAQTM1dU9l///2jePjw4V5b8lz5nDlzgvTZsmXLgsYjSXPnzg3SJ8KK397bpk2bvNvG38PvfOc7Xts222yT83WzZ8/28q5duxYzRNSAPn36ePmoUaNybptcpuecc87x8vnz50fxokWL8va71VZbRXGnTp28tn79+kXxDjvskHc/lcCRCgAgGIoKACAYigoAIJiau6by85//PIpPPfVUry3+FD4p3DWVfNJwDhMNi9+/n7yXPym5DEbc+++/7+Xx8+KXXHJJiaNDrejWrZuXf+tb3/LyDz/8MIo7d+7stX366acF95N8GumECROi+Kyzzsr5uh133LHgPsqFIxUAQDAUFQBAMDV3+iu+QrCZeW3r16+v9HD03HPPVbxPVM+jjz7q5ddff32VRoJqWLt2bd72DRs2FLxtfGXi5O3GV199tZe3b98+ilesWOG1jRs3LopffPHFvH1WAkcqAIBgKCoAgGAoKgCAYFJ/TWWPPfbw8t122y2KFy9e7LU98MADZRlDcikW1J5Zs2Z9YyxJhx56aKWHgxoVf3qn5F/jlaQBAwZEcfyJjJL0xhtvePmgQYOiuHXr1nn7feyxx6L4pz/9qdf23nvv5X1tpXGkAgAIhqICAAiGogIACCb111SOOOIIL48vAZ08l/jVV1+VZQzx6zjJftN2PhPfrK6uLoqT57qLuaZy++23BxsTak98Hkn+slGS1Ldv3yg+7LDDvLZkHpd8bMLQoUO9fNq0aVFcrt9zoXCkAgAIhqICAAgm9ae/+vfv7+XOuShOPukxlOTT/ZK3DX7yySdl6ReVMWXKFC+/4oorCn7tQw895OXHH398iCGhRu28885e3rx5ab9Sn3nmGS9PztFawpEKACAYigoAIBiKCgAgmNRdU0lez+jRo4eXx5e7X7ZsmdcWv91YkjZu3FjSGOJLUkv/flvzb37zm5L2i3Q4+uijvTz5CIV8jjvuOC+Pn/s++eSTvbZS5x/S67LLLvPy2267zcu32OJfn9Offvppr+3Pf/6zl8eXW0nemrxo0SIvjz/5Me04UgEABENRAQAEQ1EBAASTumsqF110kZe3a9fOy+N/p/LOO+94bcml8GfMmBHFjz/+uNeWzPOJ94nal7xGlnx/40uUJ+ffXnvt5eXHHntsFE+aNMlr+9GPfuTl1XjcNRov/viN5N/GJa/Hxa+jnHnmmV5b8vHCCxYsiOLkY6qTc+fBBx+M4q+//rqAUVcPRyoAgGAoKgCAYFJ3+iu5InDy8DJ+G17yFuL99tvPy+OnJs4444y8/c6fPz+K47cFftMYPvrooyhOLtOwfPnyvP2gOjp16hTF++yzT95tx4wZE8XJU1rjxo3z8osvvjiK+/Tp47X99re/9fL47agffPBBAyNGWlx66aVR3KJFC68teTrsxhtvLHi/8ac5xk9vSdLAgQO9PL6MywknnFBwH9XAkQoAIBiKCgAgGIoKACCY1F1TSUre7hm/Fbihpy62bds2iq+//vq82+6www5RnDw3nhzDqFGjcu63a9euUfy3v/0tb5+onPbt239j/E3++te/5my75JJLvDx+ve3CCy/02uJPAZT86zrxeSJJn3/+ed4xoXJ69uzp5fE/c7jzzju9tmKuoeQzduxYL0/OnfiS+slrvvX19UHGEApHKgCAYCgqAIBgKCoAgGBSd01l4cKFXj558mQvj/+NSENWr14dxT/5yU8Kft3KlSu9fNasWV5++OGHF7wvpMMXX3wRxcnrF23atCl5v/G/PXn55Ze9tuQjEvbdd98ovu+++7y2wYMHR/GaNWtKHg8ar1u3bl7eqlWrKC7Xe/PWW295+ezZs728d+/eUZz8/RNfjioNOFIBAARDUQEABJO601933XVX3rxc9t9//yhOrkx7//33V2QMKJ85c+ZEcfKW4SOPPNLLzznnnCgeNmxY3v3GV4yNL7shSSeddJKXn3vuuVF8yimneG2//vWvo3j69Ol5+0TT99xzz3l5/PRXcl5x+gsA0GRRVAAAwVBUAADBpO6aSrUkl2aJK+YpkUi/+BP3pH+/ptK9e/co7ty5s9eWvPUzn3vvvdfL49dUkF4vvPCCl8eXZaqUfMvb//3vf6/gSIrHkQoAIBiKCgAgGE5/ZQ0ZMiSKlyxZ4rUl/6Iete3KK6/08tatW3v56aefHsVTp0712n7/+997+bRp03L288Mf/rDEEaKakqt6xFcbj6+gIEmLFi3y8okTJxbcT3yF65/97GdeW/wWYkl69913ozjtp+M5UgEABENRAQAEQ1EBAATDNZWs+PnN5BMlv/rqq0oPB2W0bt06Lx89erSXx5fpOeaYY7y2+BIukjRw4MAoTj4hFLVp/fr1Xv78889H8VVXXeW1JZ/YOGDAgChOLtuT1L9//2+MpX//nXPttddGcdqfEsqRCgAgGIoKACAYigoAIBiuqWTFz4fH70tH05dceiW+tPg111zjtR1//PFe3qtXr4L7mTRpUhRPmTLFa3v99dcL3g8qK349o23btl7beeed5+XxRxokH29QjJEjR3r5k08+WfK+Ko0jFQBAMBQVAEAwVsxtkGbGPZMp5JyzhreqHuZNas1yznWp9iDySdvcadGihZcnl+KJ34LeqVMnr61Dhw5eXl9fH8XJlZCTS7HU1dUVP9jyyjl3OFIBAARDUQEABENRAQAEwzWVJoBrKigR11RQKq6pAADKj6ICAAiGogIACIaiAgAIhqICAAiGogIACIaiAgAIhqICAAiGogIACIaiAgAIptgnP66StKQcA0HJdq/2AArAvEkn5g5KlXPuFLX2FwAA+XD6CwAQDEUFABAMRQUAEAxFBQAQDEUFABAMRQUAEAxFBQAQDEUFABAMRQUAEAxFBQAQDEUFABAMRQUAEAxFBQAQTJMuKma2h5k5Myt2if8QfS82s2Mr3S/CYO6gVJv73Gl0UTGzM8zsDTNbZ2Yrs/ElZmYhBlguZrY29lVvZnWx/D+L3Nd9ZjYy8Ph2MLP/M7PVZvYPM5sYcv9pwNwJP3fM7GeJ8dVlx9guVB9pwNwpy9zZycx+b2bLs0Vxj1L206iiYmZXS7pd0q2SdpTUQdJFko6UtFWO1zRrTJ+hOOdabfqS9IGkvrF/i36BV+PTRtYTklYo8zCc9pJ+WaVxlAVzp2xjG5UY32hJ05xzqyo9lnJh7pRNvaTnJZ3aqL0450r6krStpHWSTm1gu/sk3SXp2ez2x0raT9I0SaslzZfUL7b9NEnnx/JzJU2P5U6ZCfSupH9I+h/962FjzZT55btK0iJJl2a3b97AGBdLOjYb95L0oaShyvxSfzA5htg49pY0RNJXkjZKWitpcmyf10iaK2mNpEckbV3gz/b47Oublfr+pPmLuVO+uZPoxyS9L2lQtd9z5k7tzB1lngjsJO1RynvUmCOVIyS1kPRUAdueJekmSa0lvSFpsqQXlfkEfpmkiWa2bxF9/4ekwyQdJOk0SSdk//2CbNvBkrpIGlDEPuN2lLSdMkcJQ/Jt6Jz7X0kTJY1xmU8bfWPNp0k6UdKekg5UZpJIkrKntY7KsdvvS3pb0v1m9qmZzTSzniV+L2nE3FHZ5k5cd2U+xT9ezDeQcswdVWTulKwxRaWdpFXOua83/YOZvZYddJ2Z9Yht+5Rz7lXnXL2kzpJaSbrFObfROfeKpKclnVlE37c451Y75z6QNDW7Tynzw7zNObfUOfeZpJtL/N7qJQ1zzm1wztWVuA9JusM5tzw7lsmxcco519Y5Nz3H63ZV5mhlqjIT7VeSnmpC58WZOw0rde7EDZL0mHNubSPGkTbMnYaFmDsla0xR+VRSu/i5P+dcN+dc22xbfN9LY/HOkpZm3+hNlkjapYi+V8Ti9cpMlmjfif2W4hPn3JclvjYu1zgbUidpsXNuvHPuK+fcw8p8X0cGGFMaMHcaVurckSSZ2bck/UjS/QHGkibMnYY1au40VmOKyuuSNkj6QQHbuli8XFJHM4v3vZukZdl4naSWsbYdixjTR5I6JvZbCpfIvTGZWXJMye0ba24Z9pkmzJ3c24fSX9JnylwraEqYO7m3T4WSi4pzbrWk4ZLGmdkAM2tlZluYWWdJ2+R56RvK/LCuNbMtzayXpL6SHs62vyWpv5m1NLO9Jf1XEcN6VNLlZrarmX1b0nVFvDafOZIOMLPOZra1pP9OtH8saa9AfUnSJEnfNrNBZtbMzAYo84nq1YB9VA1zxxN67mwySNIDLnvltalg7niCz51sPy2yaYtsXpRG3VLsnBsj6SpJ10paqcw3ebcydzC8luM1GyX1k9RHmbslxkka6JxbmN1krDJ3NHyszKF7MX+fcY+kF5R5M95U5rbcRnPOvSNphKSXlLn7I3lOcryk/bPndZ8sZJ/Z+9K75+jvM2V+RtcocwfHdZJ+4JrQbaHMnUjQuZNt30VSb0kPlDTolGPuRILPHWVOvW+6BrcwmxfFmtgHGQBAFTXpZVoAAJVFUQEABENRAQAEQ1EBAARDUQEABFPUSphmxq1iKeScS/ty38ybdFrlnNuh2oPIh7mTWjnnDkcqwOar1OVEgJxzh6ICAAiGogIACIaiAgAIhqICAAiGogIACIaiAgAIhqICAAiGogIACKaov6hHxvbbbx/FkyZN8toGDx4cxe+//37FxgQAacCRCgAgGIoKACAYigoAIBiuqZRg4MCBUXzUUUflbBs2bFjFxgSgtnTr1i2KR44c6bUdcsghXt61a9cofvvtt8s7sEbiSAUAEAxFBQAQDKe/CnD00Ud7+YgRI6J4/vz5XtvYsWMrMiYAta1nz55R3KNHD69twYIFXr569epKDCkIjlQAAMFQVAAAwVBUAADBcE3lG7Rp08bL77zzTi/fZpttonjKlCleWy2d+wRQPUOHDs3ZNnPmTC//+OOPyz2cYDhSAQAEQ1EBAATD6a+s+Cmt9957z2tr166dlz/xxBNRfN1115V3YIi0bNnSy+OrRX/xxRdeW+vWrb38hhtuiOKDDz7Ya0v+NfObb75Z0vh22mknL6+vr/fyfKcwNmzYEMUrV64sqX+k2wEHHODl8TnqnKv0cMqGIxUAQDAUFQBAMBQVAEAwm+01lb333tvLf/e730Vx8hrKxIkTvXz48OFRvHHjxjKMDpvEzztPmDDBazvllFOiOHkdLPn+5hO/RiYVd37bzBr9Okl67bXXorh79+4F7we146yzzip420WLFpVxJOXFkQoAIBiKCgAgGIoKACCYzeaaSpcuXbx8zJgxXn7QQQdF8bJly7y2+N84SNLixYvDDg4FyXfNYp999il422eeecbL6+rqvDz+9y/J/c6YMaPBcW7Su3dvL99uu+1ybvvuu+8WvF80fffee2+1h1AyjlQAAMFQVAAAwTTp019t27aN4ltuucVr69Wrl5evWLEiio877jivjdNd1RNffmXQoEFe25VXXlnSPj/66CMv/+c//+nlLVq0iOJWrVp5bZ9++mnO/e68885ePn36dC+Pn/76xS9+4bWNHj06z4hRi5o1a+bl8dOqSc8++6yXf/bZZ2UZUyVwpAIACIaiAgAIhqICAAimSV1TSS59Eb+Okry9M7m8+IknnhjFCxcuLMPo0Fjr16/Pm4cSX4Y+Hjdk4MCBXr777rt7+dq1a6N48uTJXlvytmbUvvjjNCTp/PPPz7ntnDlzvPzLL78sy5gqgSMVAEAwFBUAQDAUFQBAME3qmsoFF1zg5UOGDIni5LIdY8eO9fK5c+eWb2Bosr773e9G8cUXX+y1JefciBEjonjWrFnlHRiqrl+/fgVvm1w2qpZxpAIACIaiAgAIpqZPf3Xs2NHLR40alXPbRx55xMtZFgMhDB48OIp32WWXvNsm5yCanm233TaKL7/88iqOpHo4UgEABENRAQAEQ1EBAARTc9dUttxyyyieMGGC15bvyXqnn366lw8YMKDgPn/1q1/lzFetWlXwftD0HHLIITnbkk+YTC65j6Yn/qiEQw89NO+2W2zRND/TN83vCgBQFRQVAEAwFBUAQDA1d02lffv2UZxczj7p1VdfjeKlS5cW3Efr1q29fOjQoV5+0kknRXGfPn28tuXLlxfcD2pPcm7EH0udXK58+PDhXp58bDGatuQyPUkzZ86M4o0bN5Z7OBXDkQoAIBiKCgAgmJo7/XXeeeflbFu2bJmXH3PMMVFczOFls2bNvHzYsGFefv3110fxbbfd5rWddtppBfeD2nPDDTd4eX19fRT/5S9/8drefPPNiowJ6ZE8VZ7PlClToriYJ4ymHUcqAIBgKCoAgGAoKgCAYGrumkp8mZakM88808tLvU0veevnjTfe6OXxJ7pttdVWJfWB2pRc7gebt7322svLzz777CqNJD04UgEABENRAQAEQ1EBAARTc9dU4szMy4tZiqUxPvnkk4r0AyDdktdU27RpU/BrFy1aFHo4qcCRCgAgGIoKACCYmjv9FV+KJbkK6K677urlS5YsCdJnly5dvPz73/9+FL/88stB+kA6tWzZ0subN/f/y8Sf3rdu3bqKjAnp0bFjx5JfO378+IAjSQ+OVAAAwVBUAADBUFQAAMHU3DWVhx9+OIqvuOIKry35pL14+7x587y25PL2W2+9dRQnl6++5pprvHz9+vVRfPPNNzc8aNSsk08+2cs7dOjg5fGnPY4ePboiY0J6XHTRRQVv+9BDD5VxJOnBkQoAIBiKCgAgmJo7/bVmzZooHjx4sNc2bdo0L3/ppZei+IUXXvDa2rVr5+V9+vTJ2ef8+fO9fNCgQVHM0/2atoMPPjhv+8qVK6N46tSp5R4OatiCBQuqPYSK4EgFABAMRQUAEAxFBQAQTM1dU4mbMWOGl3ft2tXLH3vssSg+55xz8u5r4cKFUXzTTTd5bU899ZSXr127tqhxonbNnj272kMAagpHKgCAYCgqAIBgKCoAgGBq+ppK0pw5c7x8n332qdJI0FQlnzaazIHNHUcqAIBgKCoAgGCa1OkvoNySTxtN5ti8nHrqqdUeQupwpAIACIaiAgAIhqICAAiGaypAHkuXLvXyuro6L2/fvn0U33333V7bhRdeWL6BASnFkQoAIBiKCgAgGIoKACAYrqkAeSQfrxB/RLUk9e7dO4rnzZtXkTEBacaRCgAgGIoKACAYK2aZCTNjTYoUcs6leqlc5k1qzXLOdan2IPJh7qRWzrnDkQoAIBiKCgAgGIoKACCYYm8pXiVpSTkGgpLtXu0BFIB5k07MHZQq59wp6kI9AAD5cPoLABAMRQUAEAxFBQAQDEUFABAMRQUAEAxFBQAQDEUFABAMRQUAEAxFBQAQzP8DPtbU6aG78vUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136dc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2103122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) #卷积层\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(20,50, kernel_size=2)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.conv3_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(200, 50)\n",
    "        self.fc2 = nn.Linear(50, 10) #全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #drop不改变维度随机删去一些数据\n",
    "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = x.view(-1, 200) #view将数据变为200维的数据\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c466eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MNISTNet()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3faa43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = [] #训练损失对应的时间点\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90cbe332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  #初始化优化器\n",
    "        output = network(data) #实例化网络\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()        #反向回传\n",
    "        optimizer.step()\t   #迭代参数\n",
    "        \n",
    "        #以下为结果输出格式调整\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #注意这里的network.state_dict()是用来保存中间的训练状态的，\n",
    "            #保存后，以后可以从任何一步开始重新训练网络\n",
    "            torch.save(network.state_dict(), 'D:/liuchaochao/桌面/中间过程/model.pth') #注意 # 保存中间过程\n",
    "            torch.save(optimizer.state_dict(), 'D:/liuchaochao/桌面/中间过程/optimizer.pth') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "451249fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(): \n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():#特使不需要回传梯度\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            #计算损失\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            #计算准确数\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989f34af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\liuchaochao\\DownLoad\\anaconda\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3059, Accuracy: 1014/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302437\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.293808\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.307121\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.290816\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.354150\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.302542\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.313314\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.302453\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.273012\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.294048\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.325958\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.315298\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.324537\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.293900\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.300088\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.304446\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.300995\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.271881\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.277681\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.306115\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.291293\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.292537\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.286393\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.291937\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.290957\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.291626\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.290421\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.291817\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.293678\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.291795\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.256439\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.277564\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.283789\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.263137\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.288389\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.215236\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.238641\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.198956\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.216735\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.226401\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.128826\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.196976\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.180961\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.129130\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.150093\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.174040\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.047402\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.032981\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.039724\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.006504\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.706475\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.818087\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.850266\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.666391\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.949970\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.617471\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.662752\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 1.639627\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.802473\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.535386\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.605784\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 1.423808\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.626694\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.725081\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.494384\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.295354\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.182127\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.441655\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.649716\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.458461\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.174412\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.197508\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.395742\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.213571\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.094800\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.149622\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.317442\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.376404\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.131628\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.935880\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.138445\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.240874\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.170044\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.001350\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.024770\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.887918\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.955885\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.181264\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.975101\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.071612\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.065757\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.018052\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.170076\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.114301\n",
      "\n",
      "Test set: Avg. loss: 0.5283, Accuracy: 8686/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.125889\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.947444\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.219483\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.994067\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.875799\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.843933\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.916680\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.873568\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.014645\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.732806\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.964555\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.959274\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.696243\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.855787\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.837826\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.966254\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.934845\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.894022\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.834010\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.926011\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.055305\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.991675\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.766966\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 1.017605\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.778917\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.874995\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.967423\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.779058\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.594362\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.755037\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.677059\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.747452\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.671309\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.871073\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.940562\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 1.029089\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.741986\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.759123\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.686756\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.816378\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.655836\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.027936\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.645043\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.708504\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.604375\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.687575\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.739901\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.809562\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.578209\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.587245\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.595129\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.512553\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.752185\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 1.067536\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.592108\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.599178\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.814643\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.785640\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.795730\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.577331\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.661698\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.820855\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.770934\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.639699\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.866111\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.877200\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.689650\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.729785\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.748313\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.871901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.791377\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.782867\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.597625\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.644302\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.710607\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.899723\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 1.023205\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.936285\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.548621\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.613390\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.374630\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.815759\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.476672\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.724518\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.688612\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.531284\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.524691\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.790178\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.512978\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.696899\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.744581\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.305958\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.484809\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.606629\n",
      "\n",
      "Test set: Avg. loss: 0.2340, Accuracy: 9331/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.545211\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.483348\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.511306\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.786365\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.640677\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.595593\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.559077\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.564769\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.546332\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.428184\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.488958\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.503675\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.542194\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.658866\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.678116\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.625940\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.447001\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.463980\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.502408\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.543720\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.791515\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.707919\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.577431\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.706665\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.456169\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.352461\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.602235\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.439831\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.495960\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.653127\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.568687\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.515651\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.509123\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.466711\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.328848\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.686877\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.697878\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.554993\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.515612\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.345128\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.389833\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.356024\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.392292\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.450230\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.400031\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.338250\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.576189\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.733954\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.491622\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.522607\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.751433\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.623004\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.611762\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.685830\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.488219\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.603232\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.554798\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.392960\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.509628\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.569325\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.523905\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.561801\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.695717\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.311488\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.458948\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.380490\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.718060\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.463754\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.507888\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.656038\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.451847\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.468870\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.791701\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.621370\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.605155\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.328570\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.635916\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.270025\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.432361\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.539492\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.757150\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.512772\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.550440\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.681901\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.265638\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.600067\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.331656\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.694311\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.634616\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.688917\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.757819\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.515127\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.413966\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.709097\n",
      "\n",
      "Test set: Avg. loss: 0.1754, Accuracy: 9453/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1583883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:\n",
      "<class 'collections.OrderedDict'>\n",
      "length:\n",
      "10\n",
      "key:\n",
      "conv1.weight\n",
      "conv1.bias\n",
      "conv2.weight\n",
      "conv2.bias\n",
      "conv3.weight\n",
      "conv3.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "value:\n",
      "conv1.weight tensor([[[[-0.2977,  0.0379, -0.0112,  0.2403,  0.1651],\n",
      "          [-0.1491, -0.0432, -0.0060,  0.1909,  0.0585],\n",
      "          [-0.1548, -0.0922,  0.0522,  0.1552,  0.2579],\n",
      "          [-0.0525, -0.1180, -0.0168,  0.1954,  0.0453],\n",
      "          [-0.1618, -0.0781,  0.1500,  0.3322, -0.0334]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1326,  0.2017,  0.2598,  0.1209, -0.1647],\n",
      "          [-0.2437,  0.0007,  0.3400,  0.2183,  0.2848],\n",
      "          [-0.1876,  0.1699,  0.0412,  0.3075,  0.1950],\n",
      "          [-0.3007, -0.0787, -0.2036, -0.0666, -0.0463],\n",
      "          [-0.2485, -0.4048, -0.2947, -0.1094, -0.0841]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2674,  0.3154,  0.1233, -0.0258,  0.2667],\n",
      "          [ 0.0476,  0.0023,  0.2484,  0.2129,  0.3101],\n",
      "          [-0.2028, -0.0469, -0.0593, -0.0403, -0.1202],\n",
      "          [-0.2556, -0.0430, -0.2105, -0.0299,  0.0201],\n",
      "          [-0.0872, -0.1497, -0.1080, -0.0187, -0.1515]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1550, -0.1016, -0.1432,  0.0162, -0.0176],\n",
      "          [ 0.1662, -0.0732, -0.1623,  0.1356, -0.0523],\n",
      "          [ 0.1445,  0.0876, -0.1434,  0.0331,  0.0212],\n",
      "          [-0.1041, -0.0713,  0.0821,  0.0560, -0.0870],\n",
      "          [-0.0644,  0.1952, -0.0064,  0.1396,  0.0964]]],\n",
      "\n",
      "\n",
      "        [[[-0.0941, -0.0846,  0.2284,  0.0618,  0.2541],\n",
      "          [ 0.0492,  0.1413,  0.2899,  0.3597, -0.0006],\n",
      "          [-0.0566,  0.1233,  0.2017,  0.2113, -0.1203],\n",
      "          [-0.2593, -0.1737,  0.0481,  0.2563, -0.1115],\n",
      "          [ 0.0012, -0.1445, -0.1592,  0.0098,  0.1815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2802, -0.0292,  0.0948,  0.2573,  0.2865],\n",
      "          [ 0.2425,  0.3084,  0.1614,  0.0622, -0.0410],\n",
      "          [-0.1514, -0.0740, -0.1102, -0.2064,  0.0659],\n",
      "          [-0.0140, -0.0624,  0.0211,  0.0320,  0.1237],\n",
      "          [ 0.0300, -0.0517,  0.1757, -0.0053,  0.0713]]],\n",
      "\n",
      "\n",
      "        [[[-0.3247, -0.2007, -0.1133,  0.0757,  0.3353],\n",
      "          [-0.1527, -0.1128,  0.1903,  0.2753,  0.2422],\n",
      "          [-0.1410,  0.0786,  0.1505,  0.2801, -0.0293],\n",
      "          [-0.1032,  0.0015,  0.3245,  0.2735,  0.1062],\n",
      "          [ 0.2369,  0.3006,  0.3781,  0.2616, -0.0358]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1430, -0.0466, -0.0840, -0.2606, -0.1676],\n",
      "          [-0.0558,  0.1941, -0.2467, -0.2882, -0.2398],\n",
      "          [-0.0578,  0.1008, -0.0933, -0.1671, -0.1313],\n",
      "          [ 0.2523,  0.0119,  0.2848,  0.2971,  0.3508],\n",
      "          [ 0.0741,  0.3257,  0.1473,  0.4159,  0.1540]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2263, -0.0545, -0.1921, -0.0788,  0.0637],\n",
      "          [ 0.2673,  0.1564,  0.0814, -0.0272,  0.3804],\n",
      "          [ 0.3633,  0.3714,  0.2934, -0.0358, -0.0120],\n",
      "          [ 0.1941, -0.0810,  0.2278, -0.2095, -0.2080],\n",
      "          [ 0.0462, -0.1200,  0.0071,  0.2106,  0.2557]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1742,  0.0936,  0.0458,  0.3217,  0.0996],\n",
      "          [-0.0975,  0.1518, -0.0157,  0.1565,  0.2003],\n",
      "          [-0.2094, -0.1870,  0.0029,  0.0109, -0.0474],\n",
      "          [-0.1387, -0.0566, -0.2085, -0.2438, -0.1992],\n",
      "          [ 0.1796,  0.1578, -0.0681, -0.0330,  0.0870]]]])\n",
      "conv1.bias tensor([-0.0013, -0.0705,  0.1698,  0.1112,  0.0878, -0.0206,  0.1267,  0.2098,\n",
      "        -0.0338,  0.1195])\n",
      "conv2.weight tensor([[[[ 3.1603e-02, -7.0887e-02,  6.5438e-02],\n",
      "          [-8.0370e-02,  9.3950e-02, -5.8490e-02],\n",
      "          [ 2.4744e-02,  2.8245e-02,  3.2915e-02]],\n",
      "\n",
      "         [[ 9.7625e-02, -3.4986e-02, -3.6860e-02],\n",
      "          [-9.1377e-02, -4.7966e-03, -4.6515e-02],\n",
      "          [-3.4959e-05, -2.8193e-02,  3.3089e-02]],\n",
      "\n",
      "         [[ 1.0216e-01,  4.4704e-02, -2.2807e-02],\n",
      "          [-5.2611e-02, -9.2503e-02,  9.2861e-02],\n",
      "          [-2.3746e-02,  1.5434e-02,  4.6596e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1412e-02, -7.7897e-02, -2.2694e-02],\n",
      "          [ 4.4709e-02,  2.7370e-02,  7.7402e-02],\n",
      "          [-7.8537e-02, -3.3695e-03, -3.1007e-02]],\n",
      "\n",
      "         [[-6.0390e-02, -5.6429e-02,  2.7111e-02],\n",
      "          [ 1.0740e-02, -5.4081e-02, -7.3591e-02],\n",
      "          [-8.8600e-02,  9.5037e-02, -2.9795e-02]],\n",
      "\n",
      "         [[-1.6299e-02, -3.1442e-02, -4.9249e-02],\n",
      "          [-9.7983e-02, -6.3865e-02,  6.5220e-02],\n",
      "          [ 1.9206e-02, -4.1913e-02, -3.8280e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2242e-01,  1.5184e-01,  1.0858e-01],\n",
      "          [ 8.4110e-02,  1.3027e-01, -2.1167e-02],\n",
      "          [ 3.3865e-02,  3.1693e-02,  2.8716e-02]],\n",
      "\n",
      "         [[-9.9634e-02, -2.9182e-02, -8.7681e-02],\n",
      "          [-1.4166e-01, -7.2089e-02, -4.1391e-02],\n",
      "          [-7.6416e-02, -5.5010e-02,  5.8736e-02]],\n",
      "\n",
      "         [[-6.5904e-02, -5.7642e-02, -4.4785e-02],\n",
      "          [ 9.6149e-03, -7.1950e-02, -9.3734e-03],\n",
      "          [ 5.4151e-02,  1.0166e-02, -5.3651e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3539e-02, -1.4579e-01, -1.1145e-01],\n",
      "          [-2.2030e-01,  1.5716e-02,  6.2259e-02],\n",
      "          [-8.8953e-02, -2.5756e-02,  1.1180e-01]],\n",
      "\n",
      "         [[-1.7358e-01, -7.0785e-02,  6.7263e-02],\n",
      "          [-1.0297e-01, -1.6850e-02, -3.5482e-04],\n",
      "          [-1.6386e-01, -1.3239e-02,  1.2782e-01]],\n",
      "\n",
      "         [[-1.3031e-01,  7.1660e-03, -2.7271e-02],\n",
      "          [ 3.8600e-02,  8.6891e-03, -1.5950e-01],\n",
      "          [ 8.7223e-02, -1.2094e-01,  4.7571e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2764e-02,  5.8353e-02, -6.4839e-02],\n",
      "          [ 5.7159e-02, -6.2564e-02, -1.1688e-01],\n",
      "          [ 2.8818e-02,  4.4798e-02,  8.3077e-02]],\n",
      "\n",
      "         [[ 2.9188e-02,  1.5002e-01,  1.1210e-01],\n",
      "          [-4.5318e-02, -7.4788e-02,  5.5950e-02],\n",
      "          [-6.1335e-02,  4.1097e-02, -1.0103e-01]],\n",
      "\n",
      "         [[ 1.2508e-01,  8.2919e-02,  4.2008e-02],\n",
      "          [ 6.6234e-02, -6.9524e-02, -1.8396e-02],\n",
      "          [-1.3864e-02, -7.1281e-02, -3.9603e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0455e-01,  5.7885e-02, -3.8221e-02],\n",
      "          [-5.9453e-02,  2.7380e-02, -8.5624e-02],\n",
      "          [ 1.1220e-01,  9.3671e-02, -8.0746e-02]],\n",
      "\n",
      "         [[-7.5333e-02, -6.8744e-02, -3.9068e-02],\n",
      "          [-2.8316e-02, -1.1979e-01, -9.0835e-02],\n",
      "          [ 3.8387e-02, -3.0458e-02, -5.6895e-02]],\n",
      "\n",
      "         [[-5.7675e-02,  8.0862e-02,  4.3055e-02],\n",
      "          [-1.7777e-02, -4.9722e-02,  3.0894e-02],\n",
      "          [ 2.9033e-02,  5.0134e-02,  3.3499e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.3013e-02,  8.9954e-02,  6.2663e-02],\n",
      "          [ 1.9412e-02,  1.2791e-01,  4.9693e-02],\n",
      "          [ 5.4706e-02,  1.2053e-01,  1.9771e-02]],\n",
      "\n",
      "         [[ 1.0291e-01, -9.6690e-02, -3.8114e-02],\n",
      "          [-3.5696e-02, -7.2503e-03, -1.3234e-01],\n",
      "          [-8.6419e-02, -9.4219e-02, -2.3963e-02]],\n",
      "\n",
      "         [[ 9.7550e-02, -9.4452e-02,  5.4002e-02],\n",
      "          [-7.7722e-02, -5.7944e-02, -3.6732e-02],\n",
      "          [ 2.0204e-02,  2.2835e-02, -1.1852e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.0894e-02,  2.7692e-02, -7.9036e-02],\n",
      "          [-4.6907e-02,  1.0884e-03, -1.8345e-01],\n",
      "          [-1.7766e-01, -1.1161e-01, -4.5320e-02]],\n",
      "\n",
      "         [[ 5.8880e-02, -4.1689e-02,  7.7258e-02],\n",
      "          [-9.9692e-02, -6.3958e-02, -6.1074e-02],\n",
      "          [-1.4091e-01, -2.7475e-02, -5.3944e-02]],\n",
      "\n",
      "         [[ 6.7300e-02,  8.1190e-02, -8.6969e-03],\n",
      "          [ 8.6936e-02, -1.2032e-01, -6.0254e-03],\n",
      "          [ 1.4980e-02, -5.8903e-02, -4.5525e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.2838e-02, -1.1781e-02, -2.0603e-01],\n",
      "          [-3.8733e-02, -1.0385e-01, -7.8740e-02],\n",
      "          [ 3.8870e-02, -7.5052e-02, -7.7240e-02]],\n",
      "\n",
      "         [[ 7.2086e-02, -1.7336e-02, -2.4210e-02],\n",
      "          [ 1.3708e-02,  7.5983e-02, -1.1695e-02],\n",
      "          [ 1.8849e-01,  1.8507e-01,  1.8083e-01]],\n",
      "\n",
      "         [[ 7.2142e-02,  5.3667e-02, -1.2697e-01],\n",
      "          [ 6.6306e-02, -4.4547e-02,  8.7038e-02],\n",
      "          [-4.7220e-02,  1.4671e-01,  4.4720e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.9473e-02,  1.8154e-01,  1.3782e-01],\n",
      "          [-9.5512e-02,  9.5962e-02,  7.7446e-02],\n",
      "          [ 2.0049e-02, -4.2019e-03,  2.9486e-02]],\n",
      "\n",
      "         [[-2.4982e-02,  8.1159e-02,  1.1880e-01],\n",
      "          [ 9.5198e-02,  1.0485e-01,  1.7350e-01],\n",
      "          [-1.1972e-01, -2.9633e-03,  1.4061e-01]],\n",
      "\n",
      "         [[-7.2724e-03,  2.4511e-02,  4.3099e-02],\n",
      "          [ 8.9995e-02,  7.4301e-02, -1.3354e-02],\n",
      "          [ 1.4506e-02,  7.8443e-02,  1.1602e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.0345e-02, -5.0199e-02,  1.1836e-02],\n",
      "          [ 8.3238e-02, -1.3879e-02, -7.6963e-02],\n",
      "          [-8.3464e-02, -4.1136e-02, -2.3831e-02]],\n",
      "\n",
      "         [[-1.7088e-01, -1.3156e-01, -1.4187e-01],\n",
      "          [ 2.1316e-02,  8.0364e-02,  1.4490e-01],\n",
      "          [ 3.8043e-02, -6.1585e-02,  8.0971e-02]],\n",
      "\n",
      "         [[-1.5185e-02, -1.1058e-01, -1.3256e-01],\n",
      "          [ 3.7146e-02, -3.1403e-02,  9.1379e-02],\n",
      "          [-1.6457e-02, -3.0510e-02,  9.6576e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.7934e-02,  1.4614e-01,  6.6942e-02],\n",
      "          [-4.3083e-03, -1.7558e-02, -7.0051e-02],\n",
      "          [ 3.1510e-03,  2.2362e-03, -1.3359e-01]],\n",
      "\n",
      "         [[-2.5611e-01, -2.1826e-01, -1.3490e-01],\n",
      "          [-2.6712e-02,  5.5224e-02,  1.3282e-01],\n",
      "          [ 1.1567e-02, -5.2979e-02,  9.3344e-02]],\n",
      "\n",
      "         [[-8.4410e-02,  2.2522e-02,  2.5065e-02],\n",
      "          [ 1.9166e-02,  5.0154e-02, -6.4016e-02],\n",
      "          [-1.2404e-02, -2.5650e-02,  1.4468e-01]]]])\n",
      "conv2.bias tensor([-0.0629, -0.0189,  0.1240,  0.0565, -0.0608, -0.1176,  0.0417, -0.0179,\n",
      "        -0.0619,  0.0173, -0.1051,  0.0812,  0.0383,  0.0736, -0.0085,  0.0326,\n",
      "        -0.1360,  0.0429, -0.0204,  0.0620])\n",
      "conv3.weight tensor([[[[-0.0462,  0.1082],\n",
      "          [ 0.0752, -0.0285]],\n",
      "\n",
      "         [[ 0.0017, -0.0983],\n",
      "          [-0.0799, -0.0355]],\n",
      "\n",
      "         [[-0.0763,  0.0271],\n",
      "          [-0.0475, -0.0241]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1318, -0.0613],\n",
      "          [ 0.0544,  0.0830]],\n",
      "\n",
      "         [[ 0.1437,  0.0999],\n",
      "          [ 0.0497, -0.1199]],\n",
      "\n",
      "         [[ 0.0024,  0.0443],\n",
      "          [-0.0616, -0.0247]]],\n",
      "\n",
      "\n",
      "        [[[-0.0386,  0.0933],\n",
      "          [-0.0078,  0.1067]],\n",
      "\n",
      "         [[-0.0538, -0.0335],\n",
      "          [ 0.0246,  0.0939]],\n",
      "\n",
      "         [[-0.0432, -0.0673],\n",
      "          [ 0.0215, -0.0096]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0583,  0.1800],\n",
      "          [-0.0439,  0.1101]],\n",
      "\n",
      "         [[ 0.0670, -0.0387],\n",
      "          [-0.1085, -0.1059]],\n",
      "\n",
      "         [[-0.0351,  0.0562],\n",
      "          [ 0.0040,  0.0296]]],\n",
      "\n",
      "\n",
      "        [[[-0.0320, -0.0785],\n",
      "          [ 0.1121,  0.0030]],\n",
      "\n",
      "         [[-0.0243, -0.0115],\n",
      "          [ 0.2351,  0.0110]],\n",
      "\n",
      "         [[ 0.0377, -0.0389],\n",
      "          [-0.0277,  0.1031]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0555, -0.0843],\n",
      "          [ 0.1160, -0.0070]],\n",
      "\n",
      "         [[-0.1068,  0.0733],\n",
      "          [ 0.0651,  0.0268]],\n",
      "\n",
      "         [[ 0.0676,  0.0545],\n",
      "          [-0.0988,  0.0152]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0447,  0.0796],\n",
      "          [ 0.0411,  0.0453]],\n",
      "\n",
      "         [[-0.0735,  0.0535],\n",
      "          [-0.0375, -0.0291]],\n",
      "\n",
      "         [[ 0.0290, -0.0575],\n",
      "          [ 0.0520,  0.0574]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0832, -0.1184],\n",
      "          [-0.0490,  0.0727]],\n",
      "\n",
      "         [[-0.0083,  0.0953],\n",
      "          [-0.0718, -0.0421]],\n",
      "\n",
      "         [[-0.0977, -0.0016],\n",
      "          [-0.0573, -0.0801]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0206, -0.0043],\n",
      "          [-0.0077, -0.0554]],\n",
      "\n",
      "         [[ 0.1256,  0.1684],\n",
      "          [ 0.2101, -0.1031]],\n",
      "\n",
      "         [[-0.1087, -0.0896],\n",
      "          [-0.0191, -0.0036]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1444, -0.0047],\n",
      "          [-0.0046, -0.0835]],\n",
      "\n",
      "         [[-0.1390,  0.0532],\n",
      "          [ 0.0238, -0.0175]],\n",
      "\n",
      "         [[-0.0211,  0.0628],\n",
      "          [-0.1005, -0.0463]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1091,  0.0060],\n",
      "          [-0.0614,  0.0866]],\n",
      "\n",
      "         [[ 0.1003,  0.0414],\n",
      "          [ 0.0339,  0.0834]],\n",
      "\n",
      "         [[-0.0293,  0.0134],\n",
      "          [ 0.0202,  0.0760]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0075,  0.1030],\n",
      "          [ 0.0884, -0.0166]],\n",
      "\n",
      "         [[-0.0794, -0.0184],\n",
      "          [-0.0693, -0.0389]],\n",
      "\n",
      "         [[-0.0579,  0.0769],\n",
      "          [ 0.0558,  0.0339]]]])\n",
      "conv3.bias tensor([ 0.0775,  0.1124, -0.0932,  0.0306, -0.0172,  0.0819,  0.0353,  0.0184,\n",
      "        -0.0339, -0.0077, -0.0343,  0.0784,  0.0638, -0.0168, -0.0792, -0.0054,\n",
      "         0.0896, -0.0008,  0.0229, -0.0578,  0.0278,  0.1155, -0.0246, -0.0236,\n",
      "         0.0036, -0.0941, -0.0079, -0.0060,  0.1097, -0.0201,  0.1070, -0.0909,\n",
      "        -0.0436, -0.0109, -0.0126, -0.1142, -0.0709, -0.0598,  0.0563,  0.0572,\n",
      "        -0.0352, -0.1385,  0.0369, -0.0671,  0.1145, -0.1143, -0.0172,  0.0349,\n",
      "         0.0767, -0.0350])\n",
      "fc1.weight tensor([[-0.0582, -0.0316,  0.0580,  ...,  0.0209,  0.0813, -0.1018],\n",
      "        [ 0.0067,  0.0154, -0.0123,  ...,  0.0871,  0.0776, -0.0133],\n",
      "        [ 0.0333,  0.0188, -0.0381,  ..., -0.0650,  0.0789,  0.0115],\n",
      "        ...,\n",
      "        [-0.0566,  0.0763,  0.0181,  ..., -0.0796, -0.0383,  0.0485],\n",
      "        [ 0.0163, -0.0166,  0.0053,  ..., -0.0224,  0.1349,  0.0173],\n",
      "        [-0.0281,  0.0600, -0.0132,  ..., -0.0186, -0.0012, -0.0188]])\n",
      "fc1.bias tensor([-0.0206,  0.0655, -0.1013,  0.0990, -0.0545,  0.0650,  0.0452,  0.0662,\n",
      "         0.0399,  0.1077,  0.0695,  0.0793,  0.0508, -0.0102,  0.1432,  0.0007,\n",
      "        -0.0559, -0.0422, -0.0268, -0.0105, -0.0609, -0.0506, -0.0620,  0.0702,\n",
      "         0.0632,  0.0324,  0.0150, -0.0891,  0.1668,  0.0605, -0.0388,  0.0199,\n",
      "        -0.0525,  0.0161,  0.0440,  0.0132, -0.0196,  0.0486,  0.0400, -0.0332,\n",
      "        -0.0821, -0.0715,  0.0241,  0.0290,  0.0073, -0.0048,  0.0221,  0.0598,\n",
      "         0.1374, -0.0597])\n",
      "fc2.weight tensor([[-0.0807, -0.1177,  0.2200, -0.1244, -0.1245, -0.2503, -0.0994, -0.1509,\n",
      "         -0.1415,  0.1081, -0.0643, -0.0747, -0.1431, -0.2279,  0.1730,  0.1765,\n",
      "          0.0348, -0.0783, -0.0385,  0.0733,  0.0998,  0.1934,  0.0114, -0.2661,\n",
      "          0.0981,  0.0382, -0.1999,  0.2433, -0.0841, -0.1274, -0.1155,  0.1395,\n",
      "         -0.1651,  0.1166, -0.1860, -0.1627, -0.1318,  0.2177,  0.1558, -0.1702,\n",
      "          0.2492,  0.1497,  0.1458, -0.1530,  0.1127,  0.1463, -0.0285,  0.1159,\n",
      "         -0.1877,  0.1156],\n",
      "        [-0.1182,  0.1564, -0.2205,  0.3480, -0.1741,  0.1555,  0.1003,  0.1235,\n",
      "         -0.1064,  0.2467,  0.1620,  0.0669,  0.0966, -0.0286,  0.1838, -0.0741,\n",
      "          0.0597,  0.0937, -0.1461, -0.2073, -0.0282, -0.2166, -0.1171,  0.1666,\n",
      "          0.1313, -0.2965,  0.0567, -0.1441,  0.3056,  0.1734, -0.1116, -0.0746,\n",
      "          0.0650, -0.0688, -0.1315, -0.1137, -0.0399,  0.1076, -0.2340, -0.0963,\n",
      "         -0.2149, -0.2045, -0.1456,  0.1417,  0.0867, -0.0657,  0.1970, -0.0777,\n",
      "          0.1959, -0.2670],\n",
      "        [ 0.0993, -0.0894,  0.1564, -0.0265, -0.0849,  0.1614, -0.0636, -0.0674,\n",
      "          0.1205, -0.1173,  0.1579,  0.0880,  0.1925, -0.1317, -0.1011,  0.1802,\n",
      "         -0.0127,  0.1064, -0.0548, -0.0262, -0.0672,  0.1821,  0.0451, -0.0500,\n",
      "          0.1257, -0.2174, -0.1454,  0.0456,  0.0419, -0.0668,  0.1861,  0.0962,\n",
      "          0.1437,  0.2212, -0.0637, -0.1776, -0.0672,  0.1359, -0.1091, -0.0914,\n",
      "         -0.0870,  0.0748, -0.0064,  0.0902,  0.0943, -0.0851, -0.0222, -0.2069,\n",
      "          0.1731, -0.1307],\n",
      "        [-0.1245, -0.1146,  0.0184, -0.0845, -0.0265, -0.0102, -0.1074, -0.1405,\n",
      "          0.2180,  0.0130,  0.1932, -0.0864,  0.2118, -0.0724, -0.0745,  0.1616,\n",
      "          0.0644,  0.1416,  0.1555, -0.1001, -0.1386,  0.0238,  0.0309, -0.0038,\n",
      "         -0.0703,  0.0076, -0.0480, -0.0332, -0.0966, -0.1087, -0.0328, -0.1980,\n",
      "          0.1712, -0.1207,  0.1650, -0.0147,  0.0874, -0.0762,  0.2041,  0.1439,\n",
      "         -0.1026,  0.1502, -0.1693, -0.0811, -0.0905,  0.1682, -0.0952, -0.1150,\n",
      "          0.0571, -0.1202],\n",
      "        [ 0.0339, -0.0466, -0.2847, -0.0884,  0.1769, -0.1288,  0.1027,  0.1074,\n",
      "         -0.0177, -0.0987, -0.1702, -0.0287, -0.0382,  0.1548, -0.2014, -0.1589,\n",
      "         -0.0632, -0.0891,  0.1781,  0.1591,  0.0382, -0.1631, -0.0404,  0.2115,\n",
      "          0.1138,  0.1379,  0.2702, -0.0449,  0.0094,  0.2862,  0.0765,  0.1268,\n",
      "         -0.0514,  0.0744,  0.1347,  0.0917, -0.2005, -0.1408, -0.0327, -0.0110,\n",
      "         -0.0949, -0.2780, -0.2295,  0.2527,  0.0214, -0.1261,  0.2267, -0.0573,\n",
      "         -0.1353, -0.2184],\n",
      "        [-0.0647, -0.0835, -0.0894, -0.0906, -0.1410, -0.2421,  0.1958, -0.0764,\n",
      "          0.0076, -0.0197, -0.0403,  0.1573, -0.0306, -0.0485,  0.1737,  0.0700,\n",
      "          0.0092, -0.0317,  0.1733, -0.0260,  0.0957, -0.1330, -0.1326, -0.1507,\n",
      "         -0.0848,  0.1568, -0.0987,  0.0316, -0.1224, -0.0416, -0.1019, -0.1583,\n",
      "         -0.0457, -0.1616,  0.1599,  0.1828,  0.1860, -0.0888,  0.1708,  0.1329,\n",
      "          0.0360,  0.1367,  0.1776, -0.0772, -0.1963,  0.1790, -0.1605,  0.1553,\n",
      "         -0.1384,  0.1028],\n",
      "        [ 0.0724, -0.0894,  0.0334, -0.0483, -0.0423,  0.0406,  0.1820,  0.1103,\n",
      "         -0.1224, -0.0659, -0.2343,  0.1728, -0.1808, -0.3303,  0.1796, -0.1468,\n",
      "         -0.1786,  0.0062,  0.0249,  0.1052,  0.0850, -0.1092,  0.0496, -0.0818,\n",
      "          0.1317,  0.1193, -0.0186, -0.0722, -0.1082,  0.2373, -0.1478,  0.1095,\n",
      "         -0.1921,  0.0900, -0.2224,  0.1302,  0.0305,  0.0759,  0.1635, -0.1732,\n",
      "          0.2169, -0.0757,  0.1913,  0.2370,  0.1069, -0.1083,  0.1356, -0.1651,\n",
      "         -0.1580,  0.0956],\n",
      "        [-0.1989, -0.0576,  0.1085, -0.0584, -0.0500,  0.0502, -0.1231, -0.1624,\n",
      "         -0.1169, -0.0088,  0.1650, -0.2350,  0.1835,  0.1615, -0.1747,  0.1449,\n",
      "          0.2855,  0.0751, -0.0113, -0.1175, -0.1169,  0.1382,  0.0840,  0.2071,\n",
      "         -0.0908, -0.1713,  0.0504,  0.2643,  0.2281, -0.2303,  0.2023, -0.2132,\n",
      "          0.1101, -0.0740,  0.0957, -0.0738, -0.0021, -0.1208, -0.1871, -0.0657,\n",
      "         -0.2364,  0.1389, -0.1741, -0.2441, -0.0900,  0.1487, -0.0166,  0.2189,\n",
      "          0.1760,  0.0832],\n",
      "        [ 0.0788, -0.1030,  0.1630,  0.0015, -0.1035,  0.0852,  0.0499, -0.1095,\n",
      "         -0.0542,  0.1135,  0.1129,  0.1827, -0.0734,  0.1325,  0.1829,  0.1270,\n",
      "          0.0250, -0.0652, -0.0958, -0.0977,  0.1088, -0.0336, -0.0030,  0.0998,\n",
      "         -0.1293, -0.0665, -0.0144, -0.1114, -0.1157, -0.0928,  0.0882, -0.1104,\n",
      "         -0.1448, -0.1075,  0.1117,  0.1772,  0.1951, -0.0117,  0.1354,  0.0625,\n",
      "         -0.0791,  0.0807,  0.0969, -0.0303,  0.1379, -0.0764,  0.0406,  0.0379,\n",
      "          0.1555,  0.0910],\n",
      "        [-0.0037, -0.0555, -0.1280, -0.1238,  0.1787, -0.2318,  0.1367, -0.1656,\n",
      "         -0.1109,  0.0719, -0.0600, -0.1568, -0.0922,  0.1723, -0.0766,  0.0172,\n",
      "          0.1104, -0.0011, -0.0720,  0.1552,  0.0551,  0.0580, -0.0602,  0.2093,\n",
      "         -0.1208,  0.1299,  0.2236, -0.0814, -0.0725, -0.0845,  0.0204,  0.0373,\n",
      "         -0.1086, -0.1655,  0.1402,  0.1942, -0.0774, -0.1298, -0.0518,  0.1011,\n",
      "          0.0027,  0.1401, -0.0757, -0.1018, -0.0787,  0.1627, -0.1412,  0.2062,\n",
      "          0.0054,  0.0186]])\n",
      "fc2.bias tensor([-0.1536,  0.5350, -0.1987,  0.3113, -0.0570, -0.1450, -0.2446, -0.2900,\n",
      "         0.2267,  0.2122])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "pthfile = r'D:/liuchaochao/桌面/中间过程/model.pth'            #.pth文件的路径\n",
    "#D:/liuchaochao/桌面/中间过程/model.pth') #注意 # 保存中间过程\n",
    "#torch.save(optimizer.state_dict(), 'D:/liuchaochao/桌面/中间过程/optimizer.pth')\n",
    "model = torch.load(pthfile, torch.device('cpu'))    #设置在cpu环境下查询\n",
    "print('type:')\n",
    "print(type(model))  #查看模型字典长度\n",
    "print('length:')\n",
    "print(len(model))\n",
    "print('key:')\n",
    "for k in model.keys():  #查看模型字典里面的key\n",
    "    print(k)\n",
    "print('value:')\n",
    "for k in model:         #查看模型字典里面的value\n",
    "    print(k,model[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b36f5aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:\n",
      "<class 'dict'>\n",
      "length:\n",
      "2\n",
      "key:\n",
      "state\n",
      "param_groups\n",
      "value:\n",
      "state {0: {'momentum_buffer': tensor([[[[-0.0210, -0.0177, -0.0091,  0.0354,  0.0431],\n",
      "          [-0.0294, -0.0067,  0.0071,  0.0251,  0.0200],\n",
      "          [-0.0228, -0.0081, -0.0021,  0.0291,  0.0211],\n",
      "          [-0.0053,  0.0069,  0.0192,  0.0475,  0.0326],\n",
      "          [ 0.0170,  0.0109,  0.0209,  0.0543,  0.0589]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0528,  0.0375,  0.0358, -0.0145, -0.0653],\n",
      "          [ 0.0206,  0.0127, -0.0011, -0.0470, -0.0765],\n",
      "          [ 0.0264,  0.0310,  0.0121, -0.0451, -0.0704],\n",
      "          [ 0.0668,  0.0475,  0.0169, -0.0256, -0.0612],\n",
      "          [ 0.0623,  0.0377, -0.0014, -0.0106, -0.0607]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0246,  0.0312,  0.0282,  0.0070, -0.0004],\n",
      "          [-0.0106,  0.0017,  0.0151,  0.0110, -0.0048],\n",
      "          [-0.0069, -0.0042, -0.0036,  0.0060, -0.0054],\n",
      "          [ 0.0168,  0.0032, -0.0027, -0.0028, -0.0172],\n",
      "          [ 0.0106, -0.0104, -0.0161, -0.0262, -0.0460]]],\n",
      "\n",
      "\n",
      "        [[[-0.0165, -0.0251, -0.0159, -0.0054, -0.0110],\n",
      "          [-0.0181, -0.0141, -0.0095, -0.0028, -0.0026],\n",
      "          [-0.0099, -0.0075,  0.0121,  0.0057, -0.0039],\n",
      "          [-0.0027, -0.0056,  0.0071,  0.0055, -0.0048],\n",
      "          [-0.0039, -0.0057,  0.0003,  0.0102,  0.0043]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0459,  0.0458,  0.0301, -0.0317, -0.0566],\n",
      "          [ 0.0364,  0.0332,  0.0422,  0.0086, -0.0049],\n",
      "          [ 0.0375,  0.0444,  0.0447,  0.0318,  0.0220],\n",
      "          [ 0.0034, -0.0005,  0.0259,  0.0191,  0.0081],\n",
      "          [-0.0233, -0.0303, -0.0020,  0.0059, -0.0105]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0226,  0.0138,  0.0095,  0.0092, -0.0121],\n",
      "          [ 0.0150,  0.0188,  0.0165,  0.0294,  0.0262],\n",
      "          [ 0.0144,  0.0110,  0.0171,  0.0187,  0.0051],\n",
      "          [ 0.0329,  0.0093, -0.0120, -0.0286, -0.0261],\n",
      "          [ 0.0160,  0.0002, -0.0333, -0.0659, -0.0624]]],\n",
      "\n",
      "\n",
      "        [[[-0.0463, -0.0407, -0.0414, -0.0276,  0.0314],\n",
      "          [-0.0253, -0.0279,  0.0130,  0.0231,  0.0280],\n",
      "          [-0.0029, -0.0109,  0.0189,  0.0328,  0.0194],\n",
      "          [-0.0250, -0.0155,  0.0386,  0.0110, -0.0121],\n",
      "          [-0.0265, -0.0258,  0.0254,  0.0051,  0.0037]]],\n",
      "\n",
      "\n",
      "        [[[-0.0215,  0.0068,  0.0240,  0.0057, -0.0240],\n",
      "          [ 0.0042,  0.0105, -0.0054, -0.0090, -0.0261],\n",
      "          [ 0.0152,  0.0045,  0.0152, -0.0298, -0.0463],\n",
      "          [ 0.0045, -0.0225, -0.0274, -0.0511, -0.0520],\n",
      "          [-0.0180, -0.0295, -0.0342, -0.0470, -0.0620]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0365,  0.0121,  0.0018, -0.0056, -0.0343],\n",
      "          [ 0.0616,  0.0467,  0.0144, -0.0114, -0.0396],\n",
      "          [ 0.0583,  0.0456,  0.0245,  0.0016, -0.0228],\n",
      "          [ 0.0362,  0.0369,  0.0284, -0.0057, -0.0104],\n",
      "          [ 0.0332,  0.0354,  0.0119, -0.0258, -0.0108]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0136,  0.0229,  0.0201,  0.0225, -0.0036],\n",
      "          [-0.0017,  0.0264,  0.0212,  0.0158, -0.0201],\n",
      "          [ 0.0174,  0.0159,  0.0157,  0.0052, -0.0155],\n",
      "          [ 0.0370,  0.0268,  0.0187,  0.0029, -0.0167],\n",
      "          [ 0.0234,  0.0084, -0.0061, -0.0186, -0.0477]]]])}, 1: {'momentum_buffer': tensor([ 0.0187, -0.0222, -0.0108, -0.0076,  0.0111, -0.0050,  0.0052, -0.0235,\n",
      "         0.0066,  0.0011])}, 2: {'momentum_buffer': tensor([[[[-2.6575e-03,  4.2959e-04, -2.2758e-03],\n",
      "          [ 6.7549e-04,  2.9403e-03, -1.4227e-03],\n",
      "          [ 4.1275e-04,  2.5132e-03, -2.4835e-03]],\n",
      "\n",
      "         [[-9.0491e-03, -9.8091e-03, -1.0012e-02],\n",
      "          [-1.8650e-04, -2.4469e-04, -1.1290e-03],\n",
      "          [ 2.2611e-03,  6.1082e-03,  4.7283e-03]],\n",
      "\n",
      "         [[-1.2070e-02, -1.1595e-02, -1.2110e-02],\n",
      "          [-7.1644e-04, -8.7376e-04, -1.8517e-03],\n",
      "          [ 1.1722e-03,  2.7949e-03,  3.0444e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5035e-04,  2.5047e-03,  2.4917e-03],\n",
      "          [ 3.7965e-03,  8.1513e-03,  9.7687e-03],\n",
      "          [ 4.2918e-03,  1.6835e-03, -8.1855e-05]],\n",
      "\n",
      "         [[-3.6732e-03, -5.0416e-03, -3.8044e-03],\n",
      "          [ 1.6892e-04,  4.5312e-04,  1.2751e-03],\n",
      "          [ 2.9056e-03,  2.7412e-03,  5.6848e-03]],\n",
      "\n",
      "         [[-7.2109e-03, -9.0650e-03, -7.0311e-03],\n",
      "          [-3.0935e-04, -7.5720e-04, -2.4001e-04],\n",
      "          [ 1.0026e-03,  1.7869e-03,  1.3669e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.6540e-03,  3.5272e-02,  2.3315e-02],\n",
      "          [-2.1429e-03,  4.0015e-02,  1.3608e-02],\n",
      "          [-5.2485e-03,  4.0241e-02,  1.9536e-02]],\n",
      "\n",
      "         [[-1.5434e-03,  1.1429e-02,  1.5980e-02],\n",
      "          [ 4.3660e-02,  5.0660e-02,  1.6506e-02],\n",
      "          [ 1.3498e-02,  5.9799e-02,  1.4705e-02]],\n",
      "\n",
      "         [[-1.5812e-03,  7.1413e-03,  1.7216e-02],\n",
      "          [ 2.1104e-02,  1.9847e-02,  1.1068e-02],\n",
      "          [ 2.4899e-02,  3.2331e-02,  2.0191e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.9743e-03, -1.3185e-02, -9.0864e-04],\n",
      "          [-1.9857e-02,  4.3602e-03,  1.0677e-02],\n",
      "          [-7.1715e-03,  1.7526e-02,  5.7698e-02]],\n",
      "\n",
      "         [[-1.2537e-02, -8.5302e-03,  8.6376e-03],\n",
      "          [-2.5818e-02, -2.9878e-03,  2.1839e-02],\n",
      "          [-3.9462e-02, -3.6190e-02,  2.7650e-02]],\n",
      "\n",
      "         [[-6.2316e-03, -2.5773e-03,  1.9367e-02],\n",
      "          [ 7.4735e-03,  9.7369e-04,  8.9510e-03],\n",
      "          [ 2.5792e-02,  3.0083e-02,  2.0705e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0993e-02,  1.1915e-02,  7.1931e-03],\n",
      "          [ 4.1650e-03,  9.5520e-03,  1.4825e-02],\n",
      "          [ 2.1602e-03,  9.3205e-03,  1.8325e-02]],\n",
      "\n",
      "         [[ 5.8384e-03,  1.9168e-02,  2.2717e-02],\n",
      "          [-9.5700e-03,  6.2037e-03,  1.4347e-02],\n",
      "          [-2.1831e-03,  2.2982e-04,  6.7743e-04]],\n",
      "\n",
      "         [[-4.0626e-03,  9.1689e-03,  1.2692e-02],\n",
      "          [-1.7788e-02,  3.7355e-04,  1.3982e-02],\n",
      "          [ 1.6246e-03,  2.1470e-03, -7.7915e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8926e-03,  9.5608e-04,  1.9001e-02],\n",
      "          [-5.3596e-03, -1.5238e-03,  6.5355e-03],\n",
      "          [-2.0940e-02, -1.9595e-02,  4.1942e-03]],\n",
      "\n",
      "         [[-2.7926e-02, -6.7372e-03,  1.8145e-02],\n",
      "          [ 2.0889e-03, -2.2301e-03,  1.1617e-02],\n",
      "          [-8.6852e-03, -1.2258e-02,  6.2204e-03]],\n",
      "\n",
      "         [[ 3.5657e-03,  1.1890e-02,  5.0092e-03],\n",
      "          [-1.0152e-02,  1.1723e-03,  1.0381e-02],\n",
      "          [-1.2815e-03, -2.2021e-03, -4.3511e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.5125e-02,  1.6474e-02, -1.2347e-02],\n",
      "          [ 2.6756e-02,  1.9725e-02, -2.3601e-02],\n",
      "          [ 2.6984e-02,  1.8200e-02, -2.1426e-02]],\n",
      "\n",
      "         [[ 1.9782e-02,  2.5085e-02,  1.0242e-02],\n",
      "          [ 9.8659e-03,  1.1921e-02,  8.5840e-03],\n",
      "          [ 1.6143e-02,  1.9331e-02, -4.0589e-03]],\n",
      "\n",
      "         [[ 3.4781e-03,  1.0593e-02,  3.0600e-03],\n",
      "          [ 1.6763e-02,  1.9178e-02,  1.8200e-02],\n",
      "          [ 1.0188e-03, -5.3025e-03,  2.8593e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3499e-02, -2.0103e-02, -7.6355e-03],\n",
      "          [-2.0945e-02, -2.0639e-02,  5.6270e-03],\n",
      "          [-2.6444e-03,  1.6474e-02,  3.7377e-02]],\n",
      "\n",
      "         [[ 5.2816e-03,  2.2139e-02,  3.2843e-02],\n",
      "          [-2.3550e-02, -1.8228e-02,  8.5085e-03],\n",
      "          [-4.5947e-02, -5.5680e-03,  2.6329e-02]],\n",
      "\n",
      "         [[ 1.7073e-03, -2.8432e-03, -2.3567e-03],\n",
      "          [ 5.4816e-03,  1.1625e-02,  1.6086e-02],\n",
      "          [ 1.5118e-03, -1.2373e-02, -5.8153e-05]]],\n",
      "\n",
      "\n",
      "        [[[-3.0791e-02, -1.1689e-02, -2.4395e-02],\n",
      "          [-1.6028e-02, -6.5295e-04,  3.6477e-04],\n",
      "          [-1.0394e-02,  8.3861e-03,  1.5536e-02]],\n",
      "\n",
      "         [[ 1.8616e-02,  1.4112e-02, -7.5918e-03],\n",
      "          [ 3.4945e-02,  2.3655e-02, -7.6758e-03],\n",
      "          [-2.0490e-02,  1.3026e-02,  4.0102e-02]],\n",
      "\n",
      "         [[ 2.2714e-02,  1.4767e-02,  4.8869e-03],\n",
      "          [ 5.0615e-02,  1.9884e-02, -2.9581e-02],\n",
      "          [ 4.5818e-03,  1.9359e-02,  1.3022e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.3836e-02,  4.3588e-03,  2.0546e-02],\n",
      "          [-5.4269e-03,  1.0579e-02,  4.1839e-02],\n",
      "          [ 2.3586e-02, -2.9983e-02, -6.0503e-02]],\n",
      "\n",
      "         [[ 5.0849e-02, -2.5507e-03,  3.9892e-03],\n",
      "          [ 3.0019e-02, -1.1709e-02, -1.1856e-02],\n",
      "          [-1.3116e-02, -1.5796e-02,  1.2830e-03]],\n",
      "\n",
      "         [[-1.1991e-03,  1.9598e-02,  1.9820e-02],\n",
      "          [ 3.6410e-02,  1.0653e-02, -3.6105e-02],\n",
      "          [ 1.5164e-02,  1.9985e-02,  1.7132e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.9776e-03,  4.6076e-02,  4.3981e-02],\n",
      "          [-1.7086e-03,  4.9619e-02,  2.5617e-02],\n",
      "          [ 1.2653e-02,  2.9330e-02, -3.1724e-04]],\n",
      "\n",
      "         [[-9.3968e-04,  4.8778e-03,  9.2778e-03],\n",
      "          [-4.7021e-02, -2.0657e-02,  4.1178e-04],\n",
      "          [-4.0365e-02, -2.0568e-02,  2.3266e-02]],\n",
      "\n",
      "         [[ 5.5832e-04,  6.4683e-03,  6.0111e-03],\n",
      "          [-3.0785e-02, -2.3263e-02, -1.0655e-02],\n",
      "          [-5.5368e-02, -2.3411e-02,  1.2108e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.9423e-02, -4.4459e-02,  5.5982e-03],\n",
      "          [-1.2429e-02, -1.1157e-02,  1.5634e-02],\n",
      "          [ 2.2117e-03, -2.3114e-02, -3.4067e-02]],\n",
      "\n",
      "         [[-4.4542e-02, -2.5557e-02,  1.4243e-02],\n",
      "          [-7.7253e-02, -5.9769e-02, -2.1826e-02],\n",
      "          [-2.8932e-02, -2.6636e-02,  1.6233e-02]],\n",
      "\n",
      "         [[-8.3155e-03, -5.4666e-03,  1.0463e-02],\n",
      "          [-1.3664e-02, -3.2917e-04,  3.9185e-03],\n",
      "          [-3.7763e-02, -7.9666e-03,  1.6808e-02]]]])}, 3: {'momentum_buffer': tensor([ 0.0002,  0.0076, -0.0037, -0.0032,  0.0118, -0.0049, -0.0007,  0.0075,\n",
      "        -0.0039, -0.0225, -0.0138, -0.0017, -0.0129,  0.0083,  0.0005, -0.0176,\n",
      "        -0.0143, -0.0004,  0.0016, -0.0047])}, 4: {'momentum_buffer': tensor([[[[ 8.6974e-05,  4.1189e-05],\n",
      "          [ 2.4784e-04, -4.3101e-03]],\n",
      "\n",
      "         [[-2.0160e-02,  1.8878e-03],\n",
      "          [-2.4057e-02,  3.3147e-03]],\n",
      "\n",
      "         [[ 9.0143e-03,  4.4200e-03],\n",
      "          [ 2.3015e-02,  7.5926e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.7256e-03,  1.9140e-02],\n",
      "          [ 1.8411e-02,  1.3042e-02]],\n",
      "\n",
      "         [[ 7.6025e-03, -1.3266e-02],\n",
      "          [ 2.2916e-02, -2.5116e-02]],\n",
      "\n",
      "         [[-1.2029e-02, -3.4766e-02],\n",
      "          [ 4.6134e-04,  4.1937e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.7472e-04, -9.1546e-05],\n",
      "          [ 2.5401e-04, -1.3287e-03]],\n",
      "\n",
      "         [[-3.8969e-03,  4.4356e-03],\n",
      "          [ 1.6728e-02,  1.2737e-02]],\n",
      "\n",
      "         [[ 1.4750e-02,  2.7257e-03],\n",
      "          [-2.2967e-03, -6.7743e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5259e-02,  1.1533e-02],\n",
      "          [ 1.6142e-02, -1.7200e-03]],\n",
      "\n",
      "         [[-2.0321e-02, -1.1506e-03],\n",
      "          [-1.0283e-02, -2.5524e-03]],\n",
      "\n",
      "         [[-1.0002e-02, -3.1697e-02],\n",
      "          [-5.5397e-03, -1.7412e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.7374e-03, -2.7510e-03],\n",
      "          [-4.2351e-04,  2.4987e-04]],\n",
      "\n",
      "         [[ 2.7426e-02,  5.5303e-02],\n",
      "          [ 6.0442e-03,  1.5091e-02]],\n",
      "\n",
      "         [[-1.0552e-02,  2.4103e-03],\n",
      "          [ 4.6860e-03, -8.0426e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.3782e-02, -2.7895e-02],\n",
      "          [-5.5451e-04,  1.9091e-03]],\n",
      "\n",
      "         [[ 3.5776e-02,  1.3132e-02],\n",
      "          [-5.0862e-02,  8.7561e-04]],\n",
      "\n",
      "         [[-4.0828e-03, -9.9979e-03],\n",
      "          [ 1.5565e-02,  2.0676e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.5446e-04, -2.8767e-04],\n",
      "          [-6.6270e-04,  9.5693e-04]],\n",
      "\n",
      "         [[ 2.3958e-03, -2.7711e-04],\n",
      "          [ 5.1027e-03, -4.6065e-05]],\n",
      "\n",
      "         [[ 1.2189e-02,  5.3849e-03],\n",
      "          [ 6.3327e-03,  6.5915e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6794e-03,  2.0701e-02],\n",
      "          [ 1.6883e-02,  1.9209e-02]],\n",
      "\n",
      "         [[ 5.1826e-02,  8.3591e-02],\n",
      "          [ 1.8153e-02,  1.6699e-02]],\n",
      "\n",
      "         [[-3.1164e-03, -4.1307e-03],\n",
      "          [ 1.1796e-02,  8.1629e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.6372e-03, -2.1285e-03],\n",
      "          [-1.0007e-05, -2.0885e-04]],\n",
      "\n",
      "         [[ 4.1068e-03,  4.8930e-02],\n",
      "          [ 1.4711e-02,  1.5134e-02]],\n",
      "\n",
      "         [[-3.0183e-03, -5.2237e-03],\n",
      "          [ 9.6565e-03,  1.2124e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5989e-02,  1.3810e-03],\n",
      "          [-1.1305e-02,  1.7236e-03]],\n",
      "\n",
      "         [[ 1.7820e-02,  2.7819e-02],\n",
      "          [-4.9562e-02, -9.1565e-03]],\n",
      "\n",
      "         [[-9.6514e-03,  2.7531e-03],\n",
      "          [-5.4715e-03,  2.8808e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0503e-04,  8.1414e-05],\n",
      "          [ 5.4391e-04, -1.7295e-05]],\n",
      "\n",
      "         [[ 1.7571e-03,  1.0637e-02],\n",
      "          [ 1.2847e-02, -1.0125e-02]],\n",
      "\n",
      "         [[-5.5840e-03, -4.6902e-03],\n",
      "          [-2.6792e-03, -9.0693e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.2820e-03, -2.3462e-02],\n",
      "          [-1.9160e-02, -2.2693e-02]],\n",
      "\n",
      "         [[-3.3756e-02, -1.0176e-02],\n",
      "          [-1.5349e-02,  1.6257e-04]],\n",
      "\n",
      "         [[ 1.5698e-03,  1.2452e-02],\n",
      "          [-2.3959e-03,  2.8682e-03]]]])}, 5: {'momentum_buffer': tensor([-6.0611e-03, -1.1121e-03, -2.4375e-03, -5.0520e-03, -1.7090e-03,\n",
      "        -1.1205e-03,  4.1202e-03,  7.5602e-03,  1.3643e-03, -3.2880e-03,\n",
      "        -2.7654e-03,  3.4271e-03,  6.6979e-04,  1.1117e-02, -2.9928e-03,\n",
      "         2.2304e-03,  1.3483e-03,  4.9832e-03, -5.5800e-03,  1.1445e-03,\n",
      "        -3.5920e-05,  4.0373e-03, -5.6215e-03,  8.2285e-03,  1.8657e-03,\n",
      "         1.7040e-03,  7.8483e-04,  1.1234e-03, -1.8956e-03,  9.1254e-03,\n",
      "         4.6184e-03, -3.3384e-03, -8.7916e-04,  5.5111e-03, -1.2356e-02,\n",
      "        -3.6412e-04,  6.1877e-04, -2.4469e-04,  8.2448e-05,  1.9246e-03,\n",
      "         1.9630e-04, -1.7389e-03,  1.3870e-02, -1.1634e-02, -4.5330e-03,\n",
      "         5.2912e-04,  1.4856e-03,  2.7623e-03,  4.2299e-04, -7.9846e-03])}, 6: {'momentum_buffer': tensor([[ 0.0058,  0.0256,  0.0373,  ..., -0.0028,  0.0026,  0.0067],\n",
      "        [-0.0007, -0.0017, -0.0016,  ..., -0.0024, -0.0043,  0.0008],\n",
      "        [ 0.0442,  0.0344, -0.0150,  ...,  0.0032,  0.0239,  0.0016],\n",
      "        ...,\n",
      "        [-0.0285, -0.0159, -0.0421,  ...,  0.0004,  0.0030, -0.0019],\n",
      "        [ 0.0482,  0.0602,  0.0200,  ...,  0.0190, -0.0014,  0.0097],\n",
      "        [ 0.0192,  0.0227,  0.0010,  ...,  0.0251,  0.0114,  0.0186]])}, 7: {'momentum_buffer': tensor([ 0.0058, -0.0024,  0.0061, -0.0014,  0.0067,  0.0025, -0.0049, -0.0009,\n",
      "         0.0078, -0.0066, -0.0032, -0.0133,  0.0026, -0.0062, -0.0027,  0.0060,\n",
      "        -0.0009, -0.0081, -0.0012, -0.0022, -0.0017,  0.0052, -0.0012,  0.0038,\n",
      "         0.0078, -0.0027,  0.0079,  0.0044, -0.0064,  0.0084, -0.0074,  0.0055,\n",
      "        -0.0098,  0.0049, -0.0026, -0.0032, -0.0021,  0.0013, -0.0079, -0.0089,\n",
      "         0.0119, -0.0017,  0.0035,  0.0096,  0.0087,  0.0091, -0.0005,  0.0018,\n",
      "         0.0001,  0.0052])}, 8: {'momentum_buffer': tensor([[ 1.6473e-04,  4.4673e-04,  1.0456e-02,  2.3133e-05,  6.8013e-03,\n",
      "          7.3131e-03,  1.8668e-02,  5.2322e-03,  7.2152e-03,  8.1018e-03,\n",
      "          2.0385e-02,  3.0334e-02,  7.5205e-03,  5.3338e-03, -6.9915e-03,\n",
      "          7.0425e-02,  1.6390e-03,  3.7033e-03,  5.1617e-02,  3.7319e-03,\n",
      "          9.8936e-03,  3.5380e-02,  4.5635e-04,  1.1130e-03,  5.3563e-02,\n",
      "          3.2066e-02,  1.8678e-04,  1.3792e-02,  5.4741e-04, -3.6155e-03,\n",
      "          8.8498e-03, -1.7773e-03,  1.4086e-02,  8.3718e-03,  1.3004e-02,\n",
      "          1.6386e-02,  3.5151e-03,  1.2181e-02,  3.7116e-02,  1.4800e-02,\n",
      "         -1.5066e-02,  6.5263e-02,  3.7057e-02,  5.3239e-03,  5.6365e-03,\n",
      "          3.5155e-02, -3.6984e-03,  3.8491e-03,  1.8325e-03,  6.5140e-02],\n",
      "        [-5.6205e-04, -1.7783e-02, -1.1899e-02, -8.3190e-03,  1.5242e-02,\n",
      "          5.0958e-02, -7.0239e-03,  7.2846e-04,  3.2719e-03, -1.6084e-02,\n",
      "          1.2674e-01,  4.1041e-03,  1.5330e-01,  2.5135e-03, -1.0610e-02,\n",
      "         -4.5233e-03, -3.1730e-03,  3.3713e-02,  2.8215e-03,  1.4661e-02,\n",
      "          5.4044e-04, -1.4370e-04,  6.3553e-04, -4.0517e-02,  3.7413e-02,\n",
      "          1.1390e-03, -6.0574e-03,  6.5622e-04, -2.7630e-02,  1.5437e-02,\n",
      "          1.1385e-02,  5.9170e-03,  5.6003e-02,  5.0848e-04,  3.3693e-02,\n",
      "         -9.5072e-03, -2.8496e-02, -1.2073e-03,  3.3499e-03,  2.1449e-03,\n",
      "          4.6830e-03,  1.1717e-03,  4.3140e-03,  2.1485e-02, -2.5016e-02,\n",
      "          6.2016e-03,  2.8078e-02, -7.2750e-03,  1.2324e-01, -1.4701e-03],\n",
      "        [ 1.2373e-02,  5.8460e-03,  5.2809e-02,  1.6395e-03, -1.3119e-02,\n",
      "          1.9950e-01,  5.1497e-03,  1.3722e-02,  6.9750e-02,  4.0112e-03,\n",
      "          4.5416e-01, -1.7596e-02, -3.1952e-02,  8.8173e-03, -3.5610e-03,\n",
      "          1.2430e-01,  8.8180e-03, -1.1645e-02,  1.0514e-02, -1.7047e-02,\n",
      "          9.0606e-04,  1.7637e-01,  1.5914e-03,  4.5919e-02,  9.8803e-03,\n",
      "          1.0121e-04,  1.2534e-03,  1.9696e-02,  1.3365e-02,  6.9926e-03,\n",
      "          5.9155e-03, -6.9054e-02,  8.5182e-02,  1.2504e-03,  2.4725e-02,\n",
      "         -4.8677e-03,  3.6935e-03, -2.5943e-02,  1.2757e-02, -3.1449e-02,\n",
      "          1.0420e-02,  1.6972e-01,  2.0831e-02,  1.1496e-02,  2.2484e-02,\n",
      "         -4.7038e-02,  1.4094e-02,  1.1410e-02,  8.6023e-02,  4.1579e-04],\n",
      "        [-9.2088e-03,  6.8615e-04, -1.0852e-02,  5.6433e-04,  4.7419e-04,\n",
      "         -2.6103e-01, -1.9797e-03,  3.7376e-03, -5.4179e-02,  2.7041e-03,\n",
      "         -4.9461e-01,  2.2914e-02,  1.1463e-01,  3.9795e-02,  4.7128e-03,\n",
      "         -4.4649e-02, -1.1561e-02, -8.0951e-02, -5.1709e-02, -6.1770e-03,\n",
      "          1.0695e-02, -8.2083e-02,  7.6232e-03,  4.0029e-03,  2.0718e-02,\n",
      "          2.1436e-02,  1.9437e-04, -2.1042e-03,  3.8800e-03,  1.6531e-03,\n",
      "          2.2864e-03,  8.1880e-03, -1.9515e-01, -5.4212e-03, -1.1520e-01,\n",
      "          1.0972e-02,  1.7378e-03,  1.1000e-02,  1.2082e-02, -2.0220e-02,\n",
      "         -3.7737e-03, -2.0110e-01, -1.6399e-03,  2.6329e-03, -6.3091e-03,\n",
      "         -9.0712e-02,  4.6933e-03, -1.3485e-02, -1.4397e-01,  9.9197e-03],\n",
      "        [ 2.0012e-02,  4.3938e-03,  2.1239e-03,  1.8615e-03,  1.5066e-01,\n",
      "          2.9035e-04,  5.5218e-02, -1.0861e-02,  2.5223e-03,  2.5810e-03,\n",
      "         -1.1006e-02,  4.1890e-02, -2.2472e-02,  2.6525e-01,  5.0582e-03,\n",
      "          3.5618e-03, -3.5166e-04,  1.0998e-02,  2.2785e-02, -6.5921e-02,\n",
      "         -2.3911e-03,  2.1951e-03,  1.0328e-02,  9.0815e-02, -1.3379e-02,\n",
      "          1.3240e-01,  1.7532e-01,  6.9387e-04,  1.2902e-03, -7.5272e-02,\n",
      "          6.3698e-02, -2.3717e-02, -3.4950e-02,  4.2042e-03,  2.4152e-01,\n",
      "          2.4353e-02,  1.3471e-04,  1.9588e-03,  8.4096e-03,  1.1267e-01,\n",
      "          6.6908e-03, -2.2032e-03,  6.2216e-03, -8.2790e-02, -1.2375e-03,\n",
      "          9.8347e-03, -2.9328e-02,  2.0276e-02, -6.3106e-04,  6.8899e-03],\n",
      "        [-1.7239e-01,  1.9505e-03, -2.3173e-02,  3.1494e-05,  2.7758e-03,\n",
      "         -1.2442e-01, -7.5551e-03,  4.9648e-03,  1.3672e-02,  1.3386e-02,\n",
      "          1.6299e-02, -2.7125e-01,  9.5009e-03, -4.6183e-02,  1.0528e-01,\n",
      "          6.4206e-02,  4.9430e-03,  4.1955e-03, -9.8626e-02,  2.3280e-02,\n",
      "         -1.5703e-01,  2.0492e-02, -1.3642e-02,  2.3520e-03,  1.2816e-02,\n",
      "         -8.9954e-03,  4.2969e-03,  3.5800e-02,  2.0959e-03, -6.5246e-02,\n",
      "         -2.6281e-02, -1.2341e-01,  2.3358e-02, -6.1309e-05, -1.4404e-02,\n",
      "         -1.9196e-01, -7.0215e-02,  3.2266e-03, -2.0189e-01,  1.4547e-01,\n",
      "         -1.1702e-01,  8.4412e-02, -2.9902e-02,  3.8386e-03, -1.7414e-02,\n",
      "          6.1073e-02,  5.9122e-03,  7.5914e-03,  1.6942e-02,  1.6890e-01],\n",
      "        [ 9.9376e-02,  2.1759e-03, -1.6618e-03,  3.9187e-04,  1.2877e-03,\n",
      "          1.6387e-01, -1.3392e-02, -5.0110e-03, -1.9019e-03,  4.0694e-03,\n",
      "          2.6289e-03,  3.1735e-01,  1.6384e-03, -1.5760e-03, -8.6283e-02,\n",
      "         -5.5724e-02,  2.4462e-04,  7.2002e-03,  1.0255e-01, -4.1211e-02,\n",
      "          2.0755e-01,  1.0020e-02,  4.1041e-04,  4.7275e-03, -6.4637e-02,\n",
      "          4.5617e-02, -3.4698e-04, -1.9865e-03,  3.5889e-03,  1.1789e-01,\n",
      "          8.0868e-04,  2.3413e-01,  7.2542e-03, -1.2881e-02, -3.2900e-03,\n",
      "          1.3369e-01,  7.3160e-02,  1.0721e-02,  2.0834e-01,  2.4538e-02,\n",
      "          1.2023e-01, -1.5338e-02,  9.7143e-02,  4.3244e-02,  7.4955e-02,\n",
      "         -1.4623e-02,  2.9496e-02, -1.2210e-02,  3.9599e-03, -6.6519e-02],\n",
      "        [-1.7454e-02,  8.3191e-04, -5.5958e-02,  1.4306e-03,  6.2583e-03,\n",
      "         -3.6412e-02,  1.2492e-02, -1.8747e-02, -1.8477e-02,  2.7205e-03,\n",
      "         -1.6511e-01,  1.0320e-02, -2.3022e-01,  8.9278e-03,  2.6238e-03,\n",
      "         -1.5990e-01, -1.7226e-02, -1.2562e-02, -1.3408e-02,  8.1337e-03,\n",
      "         -6.5323e-03, -1.0914e-01, -5.7729e-03, -2.9760e-02, -6.6992e-02,\n",
      "          6.5655e-03, -2.9606e-02, -2.6232e-02, -1.0767e-02,  1.5033e-03,\n",
      "         -3.1945e-02, -2.7668e-03,  1.7285e-02, -2.9835e-03,  2.1595e-02,\n",
      "          8.1448e-03,  8.7399e-04, -1.6243e-02,  1.2547e-02,  1.2267e-02,\n",
      "         -1.6852e-02,  1.1203e-02, -7.8557e-03, -2.3157e-02, -1.0273e-02,\n",
      "          5.3951e-02, -5.6123e-02, -3.7071e-02, -1.1065e-01, -4.4242e-02],\n",
      "        [ 6.4757e-02,  1.1357e-03,  3.2207e-02,  2.2603e-03, -1.4166e-01,\n",
      "          3.3069e-02,  9.8858e-02,  5.6331e-03, -2.8441e-02, -1.8550e-03,\n",
      "          5.2563e-02, -9.8504e-02, -1.4061e-02, -3.4063e-01, -8.4610e-03,\n",
      "         -3.5687e-02,  4.7581e-03,  4.0487e-02, -1.1345e-02,  6.3089e-03,\n",
      "         -5.8055e-02, -4.9183e-02, -4.7290e-02, -2.0090e-01,  6.2227e-03,\n",
      "          2.4635e-02, -7.1315e-02, -2.3663e-03,  9.7635e-03,  4.8892e-03,\n",
      "         -1.3837e-01, -6.9177e-03,  2.4055e-02,  4.7161e-03, -2.5938e-01,\n",
      "          4.1412e-02,  2.4489e-02,  3.6578e-03, -1.3471e-01, -5.2642e-02,\n",
      "          8.8644e-03, -1.1849e-01, -1.1996e-01,  2.2126e-03, -5.2666e-02,\n",
      "          6.4347e-03, -1.5251e-02, -6.9829e-02,  2.5619e-03, -5.9957e-02],\n",
      "        [ 2.9330e-03,  3.1675e-04,  5.9472e-03,  1.1643e-04, -2.8715e-02,\n",
      "         -3.3132e-02, -1.6044e-01,  6.0069e-04,  6.5676e-03, -1.9635e-02,\n",
      "         -2.0442e-03, -3.9558e-02,  1.2120e-02,  5.7757e-02, -1.7639e-03,\n",
      "          3.7994e-02,  1.1909e-02,  4.8623e-03, -1.5199e-02,  7.4241e-02,\n",
      "         -5.5828e-03, -3.9086e-03,  4.5660e-02,  1.2225e-01,  4.3941e-03,\n",
      "         -2.5496e-01, -7.3932e-02, -3.7949e-02,  3.8661e-03, -4.2283e-03,\n",
      "          1.0366e-01, -2.0597e-02,  2.8802e-03,  2.2963e-03,  5.7729e-02,\n",
      "         -2.8628e-02, -8.8923e-03,  6.4683e-04,  4.2005e-02, -2.0758e-01,\n",
      "          1.8165e-03,  5.3606e-03, -6.2144e-03,  1.5714e-02,  9.8409e-03,\n",
      "         -2.0276e-02,  2.2129e-02,  9.6743e-02,  2.0698e-02, -7.9081e-02]])}, 9: {'momentum_buffer': tensor([ 0.0218,  0.0132,  0.0141, -0.0157,  0.0143,  0.0030,  0.0250, -0.0269,\n",
      "        -0.0242, -0.0246])}}\n",
      "param_groups [{'lr': 0.01, 'momentum': 0.5, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "pthfile = r'D:/liuchaochao/桌面/中间过程/optimizer.pth'            #.pth文件的路径\n",
    "#D:/liuchaochao/桌面/中间过程/model.pth') #注意 # 保存中间过程\n",
    "#torch.save(optimizer.state_dict(), 'D:/liuchaochao/桌面/中间过程/optimizer.pth')\n",
    "model = torch.load(pthfile, torch.device('cpu'))    #设置在cpu环境下查询\n",
    "print('type:')\n",
    "print(type(model))  #查看模型字典长度\n",
    "print('length:')\n",
    "print(len(model))\n",
    "print('key:')\n",
    "for k in model.keys():  #查看模型字典里面的key\n",
    "    print(k)\n",
    "print('value:')\n",
    "for k in model:         #查看模型字典里面的value\n",
    "    print(k,model[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
